# Руководство по дообучению модели

## Что такое дообучение (Fine-tuning)?

Система использует предобученную модель Food-101 для распознавания типа блюда, и дообучает регрессионную модель для предсказания калорий и БЖУ на основе данных от ChatGPT.

## Архитектура

```
Food-101 (предобученная, заморожена)
    ↓
[Извлечение признаков]
    ↓
Регрессионная модель (обучается)
    ↓
Предсказание: [calories, proteins, fats, carbs]
```

## Процесс работы

### Фаза 1: Сбор данных

1. Запустите API и обрабатывайте запросы
2. Интегрируйте ChatGPT (см. документацию по интеграции)
3. При каждом запросе сохраняются:
   - Изображение
   - Результаты вашей модели
   - Результаты ChatGPT (как эталон)
   - Разница между моделями

### Фаза 2: Обучение модели

Когда накопится достаточно данных (минимум 50-100 образцов):

1. Запустите обучение через API:
```bash
curl -X POST "http://localhost:8000/train-model?epochs=20&batch_size=8" \
  -H "Authorization: Bearer your-token"
```

Или через Swagger UI: http://localhost:8000/docs

2. Параметры обучения:
   - `epochs`: Количество эпох (по умолчанию 20)
   - `batch_size`: Размер батча (по умолчанию 8)
   - `learning_rate`: Скорость обучения (по умолчанию 0.001)

3. Результаты сохраняются в папку `models/`:
   - `nutrition_model_best.pth` - лучшая модель (по валидации)
   - `nutrition_model_final.pth` - финальная модель

### Фаза 3: Использование дообученной модели

После обучения модель автоматически будет использоваться в `ml_service.py` (когда реализовано).

## Требования к данным

### Минимум для обучения:
- **50 образцов** - базовое обучение (низкая точность)
- **100-200 образцов** - нормальное обучение
- **500+ образцов** - хорошее качество
- **1000+ образцов** - отличное качество

### Качество данных:
- Данные должны содержать результаты ChatGPT
- Разнообразие блюд (не только пицца)
- Хорошее качество изображений

## Проверка готовности данных

Проверьте количество данных для обучения:

```bash
curl -X GET "http://localhost:8000/stats" \
  -H "Authorization: Bearer your-token"
```

Смотрите поле `training_data_samples` - это количество образцов с данными ChatGPT.

## Мониторинг обучения

Во время обучения логи выводятся в консоль:

```
Epoch 1/20 - Train Loss: 45.2341, Val Loss: 48.1234
Epoch 2/20 - Train Loss: 38.5678, Val Loss: 42.3456
...
```

**Что смотреть:**
- `Train Loss` должен уменьшаться
- `Val Loss` должен уменьшаться (не должен расти)
- Если `Val Loss` растет - возможно переобучение (уменьшите epochs)

## Время обучения

Примерное время на CPU сервере:

- 100 образцов: ~10-20 минут
- 500 образцов: ~30-60 минут
- 1000 образцов: ~1-2 часа
- 5000 образцов: ~4-8 часов

На GPU сервере в 5-10 раз быстрее.

## Интеграция с API

После обучения модель можно интегрировать в основной сервис:

1. Модель сохраняется в `models/nutrition_model_best.pth`
2. `ml_service.py` может загружать и использовать эту модель
3. Вместо эвристик используется обученная модель

## Улучшение модели

### Периодическое переобучение:

1. Соберите больше данных
2. Запустите обучение снова
3. Новая модель заменит старую (если лучше)

### Активное обучение:

- Используйте ChatGPT только для сложных/новых случаев
- Собирайте данные о расхождениях между моделями
- Фокусируйтесь на типах блюд с большой ошибкой

## Устранение проблем

### "Нет данных для обучения"

Решение: Интегрируйте ChatGPT и соберите данные через API.

### "Мало данных для обучения"

Решение: Соберите больше образцов (минимум 50-100).

### "Ошибка загрузки модели"

Решение: Проверьте, что модель Food-101 доступна, есть интернет.

### "Out of memory"

Решение: Уменьшите `batch_size` (например, до 4 или 2).

## Настройка через .env

В файле `.env` можно настроить поведение системы:

- `USE_TRAINED_MODEL=true` - использовать дообученную модель вместо эвристик
- `USE_OWN_MODEL_INSTEAD_CHATGPT=true` - показывать результаты своей модели вместо ChatGPT
- `USE_CHATGPT=true` - использовать ChatGPT для сбора данных

## Будущие улучшения

1. ✅ **Сохранение изображений**: Реализовано - изображения сохраняются в `uploads/images/`
2. **Автоматическое обучение**: Периодическое переобучение по расписанию
3. **A/B тестирование**: Сравнение старой и новой моделей
4. **Метрики качества**: Отслеживание точности на тестовой выборке

